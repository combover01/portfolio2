---
title: LOUPE
date: 2024-03-12 17:21:00 Z
---

https://youtu.be/pKbprR2uYXI

Introduction/Motivation
For this project, I wanted to create a brain music system that plays on the idea of soundscape composition and one’s memory of a recording, inspired by the works of Hildegard Westerkamp, Annea Lockwood, Victoria Shen, and Yvette Jackson. I have had the idea of a “granularizer” for a while, similar to a granular synthesis system but with a longer loop time so it creates audio loops where you can still hear the original signal instead of creating an oscillator. This would create the opportunity to make new soundscapes out of a pre-existing recording and would force the listener to focus on parts of the recording that they might otherwise skip over, allowing deep listening to occur. In Annea Lockwood’s soundscape recording philosophy, she focuses on how a river never sounds the same twice, and nature never sounds the same even if you use the same microphone and go back to the same location. I wanted to revisit that idea from the perspective of the brain and our memories. You can never experience the same sound twice, but a memory of the sound remains and can change over time. To recreate this sort of never-the-same soundscape digitally, I wanted to use my brain waves to control a recording that I took. As Hildegard Westerkamp says, “is it possible… to create a soundscape composition, i.e. to portray a true relationship to a soundscape, a place, a situation, if the composer has not experienced it through the recording process?” (Westerkamp 55). Using my own recording, and in my case, my own performance, to be “granularized” by my brain can help me explore my memories of the moment in time in which it was recorded in a form of deep listening (term coined by Pauline Oliveros). 
Literature Review
In preparation for this project, I read about the performance and compositional practices of a few composers we learned about in class. This includes Hildegard Westerkamp (1), Yvette Jackson (2), and Victoria Shen (3). Westerkamp’s philosophies on the soundscape recording process being as important as the audio output and focus on the presence of a microphone changing the space especially inspired me. Jackson’s definitions of a narrative soundscape helped me define my performance intentions - I intended to create narrative soundscape, not just a soundscape through performance with this system. Shen’s performance technique of listening to the feedback of the room, audience, and instrument all together resonated with me in my own performance practice, and helped me realize that I could use my own recording as the audio input to the granularizer, and that brain waves can be used as a form of feedback between the outer and inner self. 
System/Design
The system is a set of software consisting of 3 Max patches and a Python file, with the audio processing only happening in one of the Max patches (I don’t like to complicate my Max audio generation systems). To control the “granularizer” Max patch, I used my brain’s most dominant frequency (with the highest amplitude from an FFT) to set the start point of the “grain” or loop, and my alpha band power (8-12 Hz brain waves, usually higher power in a state of relaxation) to control the length of the grain loop. The loop shortens by a small factor (multiplying the length in milliseconds by 0.9-0.99, set by the performer) every time it repeats, so if the performer stays at the same brain frequency for long enough time, the recording will sound more like a tone and a traditional granular synthesis tool rather than a soundscape generator. If the brain’s dominant frequency changes, the length will be reset to the value set by the alpha band power. The total range for the loop length was between 0.5ms and 1 minute, but typically stayed in the 20-45 second range as set by alpha power. 
I used the OpenBCI Cyton board to send 8-channel EEG to the OpenBCI GUI software, which I configured to send average band power values and FFT values of the brain signal to Max. Since it was a 125-bin FFT for 8 channels of EEG, the UDP message was too long to be received in Max so I had to receive it in Python - which turned out to be a good problem to have because I could much more easily process the data in Python than in Max. I took the average of all the channels’ frequencies and found the frequency bin with the highest amplitude and that was the value sent to Max via another UDP stream.
Challenges/Successes
For this project, I wanted to design my own brain band power/FFT object that would output the various frequency component’s amplitude levels from a LSL (Lab Streaming Layer) stream of the user’s EEG. This would interface with my LSL upsampling external that I am making for my grad research and would be a great next step for my research project. Unfortunately, I couldn’t get it working in time so I used the OpenBCI GUI to send UDP streams of the data I needed as it was somewhat more reliable. As mentioned earlier, I had to rig up a Python UDP receiver/transmitter to process the FFT data because it was too long, but once I got it working, it was an elegant solution. Using the udpreceive object is one of the best ways to get software to talk to Max, so this was a success for any future projects as well - I will start doing a Python UDP receiver/processor/transmitter file more often! Another challenge related to FFT processing I had was that because the sampling rate is so low for EEG signals (250Hz with my sensor), the frequency resolution is low, so each frequency bin in the FFT was slightly less than 1Hz (4). Due to the noisy nature of EEG signals, this made the signals vary less than I had intended - and often they would vary from ~12Hz being the dominant frequency all the way to 50Hz instantaneously, where the higher frequencies were noise and not useful brain signals. So, I capped the frequency measurement at 40Hz. Ideally, I could do some more slope calculation to make sure that the dominant frequency is actually the dominant brain rhythm and not a jump from one noisy signal from one channel to the next noisy signal, but this will be a future implementation. 
I got to perform a soundscape piece with the system live shortly after creating it, and the performance went better than expected. Here is a link to the full performance video. The Max and UDP streaming systems never failed, just the connection to the EEG amplifier, which is a common issue that I deal with when using this amp for other performances. Since it was a common issue, I made sure that I knew which parameters to control manually while it rebooted during the performance and the audience didn’t know it happened. For the piece that I performed, I used a previous solo saxophone and EEG work as the sample/soundscape to be granularized, and then performed a new saxophone improvisation on top of the newly granularized recording inspired by the licks that I heard in the loop that was chosen by my brain waves. The new saxophone on top of the old meshed well, and the greatest success of the performance was that around the 13 minute mark, my brain triggered the last few seconds of the recording, which happened to be applause from the previous performance. As I already had wanted to wrap up the performance at that point, it was a great stopping place and it solved the ambient-performance-when-is-it-over problem and made it very obvious to the audience that the piece was over.  I will present 5 of my favorite minutes of the piece in class.
Conclusion
Overall, this project helped me contextualize my brain music performance practice in a new soundscape-inspired composition. I pulled ideas from soundscape composition practices discussed by Annea Lockwood, Hildegard Westerkamp, and Yvette Jackson and inverted them to create a brain-controlled recording playback system that was stimulating to perform alongside. While I performed with a recording of myself playing ambient music, I believe it would be interesting to try with nature or ambiance sounds that would be more typical to hear in a soundscape composition. Listening to your own recording in a new light can elicit an unexpected response, discovering new complexities to the sound or helping you remember a small detail. For example, in my performance, I did not remember that the original piece had two keys until suddenly a lop started playing in a different home key than I had remembered. This blended with my reverberated saxophone in a different key to create new and interesting harmonies - some may call it dissonance, but in a soundscape we can explore the sounds present in a recording in an unbiased manner, free of equal temperament or a fixed time. 

Works Cited
1) Westerkamp, Hildegard. "Soundscape composition: Linking inner and outer worlds." Soundscape Newsletter, Amsterdam, Holland (1999).

2) Jackson, Yvette Janine. "Narrative Soundscape Composition: Approaching Jacqueline George’s Same Sun." Between the Tracks: Musicians on Selected Electronic Music (2020).

3) Shen, V. (2019, November 29). Luff 2019 - interviews - evicshen. YouTube. https://www.youtube.com/watch?v=hi4BwOEgU2Y

4) OpenBCI networking guide: https://docs.google.com/document/d/e/2PACX-1vR_4DXPTh1nuiOwWKwIZN3NkGP3kRwpP4Hu6fQmy3jRAOaydOuEI1jket6V4V6PG4yIG15H1N7oFfdV/pub

